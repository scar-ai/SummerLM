# SummerLM-4B Architecture ASCII Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                            SUMMERLM-4B ARCHITECTURE (4B PARAMETERS)             │
└─────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────┐
│                              INPUT PROCESSING                                   │
└─────────────────────────────────────────────────────────────────────────────────┘

Input Tokens [B, T] (where T ≤ 4096)
        │
        ▼
┌─────────────────┐
│  Token Embedding│  ← Embedding Matrix: [vocab_size × 3072]
└─────────────────┘
        │
        ▼ (add dropout 0.1)
┌─────────────────┐
│     Dropout     │
└─────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                            TRANSFORMER STACK (32 LAYERS)                         │
└─────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────┐    ┌─────────────────────┐    ┌─────────────────────┐
│   Transformer Block │ →  │   Transformer Block │ →  │   ... (32 blocks)   │
│        Layer 1      │    │        Layer 2      │    │      Layer 32       │
└─────────────────────┘    └─────────────────────┘    └─────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│                                OUTPUT PROCESSING                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────┐
│   RMSNorm (LN)  │
└─────────────────┘
        │
        ▼
┌─────────────────┐
│ Output Head     │  ← Linear: [3072 × vocab_size] (no bias)
└─────────────────┘
        │
        ▼
┌─────────────────┐
│    Logits       │  → Next Token Probabilities
└─────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────┐
│                          SINGLE TRANSFORMER BLOCK DETAIL                        │
└─────────────────────────────────────────────────────────────────────────────────┘

Input: [B, T, 3072]
        │
        ┌─────────────────┐
        │    Residual     │ ◄─────────────────────┐
        └─────────────────┘                       │
                │                               │
                ▼                               │
┌─────────────────┐                               │
│   RMSNorm (LN)  │                               │
└─────────────────┘                               │
                │                               │
                ▼                               │
┌─────────────────────────────────────────────────────────────────────┐
│                    MULTI-HEAD ATTENTION                          │
│   ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│   │      QKV        │  │      QKV        │  │      QKV        │  │
│   │  Projection     │  │  Projection     │  │  Projection     │  │
│   │  [3072→9216]    │  │  [3072→9216]    │  │  [3072→9216]    │  │
│   └─────────────────┘  └─────────────────┘  └─────────────────┘  │
│           │                   │                   │             │
│           ▼                   ▼                   ▼             │
│   ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│   │ Query [96]      │  │  Key [96]       │  │ Value [96]      │  │
│   │ [B,32,T,96]     │  │ [B,32,T,96]     │  │ [B,32,T,96]     │  │
│   └─────────────────┘  └─────────────────┘  └─────────────────┘  │
│           │                   │                   │             │
│           └─────────┬─────────┘                   │             │
│                     ▼                           │             │
│         ┌─────────────────────┐                 │             │
│         │   RoPE + ALiBi      │ ← Positional     │             │
│         │   Rotary Embedding  │   + Distance Bias│             │
│         └─────────────────────┘                 │             │
│                     │                           │             │
│                     ▼                           │             │
│         ┌─────────────────────┐                 │             │
│         │ Scaled Dot-Product  │                 │             │
│         │   Attention         │                 │             │
│         │ (Causal Mask)       │                 │             │
│         └─────────────────────┘                 │             │
│                     │                           │             │
│                     ▼                           │             │
│         ┌─────────────────────┐                 │             │
│         │   Output Proj       │                 │             │
│         │  [9216→3072]        │                 │             │
│         └─────────────────────┘                 │             │
└─────────────────────────────────────────────────────────────────────┘
                │                               │
                ▼                               │
┌─────────────────┐                               │
│ Layer Scale (32)│ → × [if enabled]             │
└─────────────────┘                               │
                │                               │
                └─────────────────────┐         │
                                    │         │
                ┌─────────────────┐  │         │
                │    Add          │  │         │
                │  Residual       │───┘         │
                └─────────────────┘            │
                        │                      │
                        ▼                      │
┌─────────────────┐                          │
│    Residual     │ ◄─────────────────────────┘
└─────────────────┘
        │
        ▼
┌─────────────────┐
│   RMSNorm (LN)  │
└─────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        SwiGLU MLP                               │
│   ┌─────────────────┐                                           │
│   │    Gate FC1     │  ← [3072 → 10752] (3.5× expansion)       │
│   │  Linear Layer   │                                           │
│   └─────────────────┘                                           │
│           │                                                     │
│           ▼                                                     │
│   ┌─────────────────┐  ┌─────────────────┐                      │
│   │   SiLU Gate     │  │   Linear G2     │                      │
│   │ Activation      │  │   [3072→10752]  │                      │
│   └─────────────────┘  └─────────────────┘                      │
│           │                     │                               │
│           └─────────┬───────────┘                               │
│                     ▼                                           │
│         ┌─────────────────────┐                               │
│         │   Element-wise      │                               │
│         │   Multiplication    │                               │
│         └─────────────────────┘                               │
│                     │                                           │
│                     ▼                                           │
│         ┌─────────────────────┐                               │
│         │    Output FC2       │ ← [10752 → 3072]               │
│         │    Dense Layer      │                               │
│         └─────────────────────┘                               │
│                     │                                           │
│                     ▼                                           │
│         ┌─────────────────────┐                               │
│         │     Dropout 0.1     │                               │
│         └─────────────────────┘                               │
└─────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────┐
│ Layer Scale (32)│ → × [if enabled]
└─────────────────┘
        │
        ▼
┌─────────────────┐
│    Add          │
│  Residual       │
└─────────────────┘
        │
        ▼
Output: [B, T, 3072]

┌─────────────────────────────────────────────────────────────────────────────────┐
│                      ATTENTION MECHANISM DETAIL                                 │
└─────────────────────────────────────────────────────────────────────────────────┘

Multi-Head Attention (32 heads, 96 dims each):

┌─────────────────────────────────────────────────────────────────────┐
│                     INPUT: [B, T, 3072]                            │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                       QKV PROJECTION                               │
│                   Combined Linear Layer                             │
│                    [3072 → 9216]                                   │
│                (bias=False, efficient fused)                       │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                       RESHAPE & TRANSPOSE                          │
│              → Q, K, V: [B, T, 3072] each                          │
│              → View: [B, T, 32, 96]                               │
│              → Transpose: [B, 32, T, 96]                          │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                  ROTARY POSITIONAL EMBEDDINGS (RoPE)              │
│                                                                     │
│  ┌─────────────────┐    ┌─────────────────┐                       │
│  │  Query RoPE     │    │   Key RoPE      │                       │
│  │  Apply rotate   │    │  Apply rotate   │                       │
│  │  (cos/sin)      │    │  (cos/sin)      │                       │
│  └─────────────────┘    └─────────────────┘                       │
│                                                                     │
│  Rotation for each position ensures relative position awareness    │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│              SCALED DOT-PRODUCT ATTENTION                          │
│                                                                     │
│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐ │
│  │   Q   •  K^T    │ →  │   Scale:        │ →  │  Add ALiBi      │ │
│  │   [32]          │    │   1/√96 ≈ 0.1   │    │  Distance Bias  │ │
│  │   Scores        │    │                 │    │  (Learned)      │ │
│  └─────────────────┘    └─────────────────┘    └─────────────────┘ │
│           │                       │                   │           │
│           └───────────┬───────────┘                   │           │
│                       ▼                               │           │
│           ┌─────────────────────┐                     │           │
│           │   Apply Causal      │                     │           │
│           │     Mask            │                     │           │
│           │   (Upper Tri)       │                     │           │
│           └─────────────────────┘                     │           │
│                       │                               │           │
│                       ▼                               │           │
│           ┌─────────────────────┐                     │           │
│           │     Softmax         │                     │           │
│           │  (Attention Weights)│                     │           │
│           └─────────────────────┘                     │           │
│                       │                               │           │
│                       ▼                               │           │
│           ┌─────────────────────┐                     │           │
│           │ Dropout (0.1)       │                     │           │
│           │  During Training    │                     │           │
│           └─────────────────────┘                     │           │
│                       │                               │           │
│                       └─────────────────────┐         │           │
│                                             │         │           │
│           ┌─────────────────┐                │         │           │
│           │   Multiply V    │────────────────┘         │           │
│           │   Weighted Sum  │                          │           │
│           └─────────────────┘                          │           │
│                     │                                  │           │
│                     ▼                                  │           │
│           ┌─────────────────────┐                      │           │
│           │   Attention Output  │                      │           │
│           │  [B, 32, T, 96]     │                      │           │
│           └─────────────────────┘                      │           │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    RESHAPE & OUTPUT PROJECTION                      │
│              → Transpose: [B, T, 32, 96]                           │
│              → View: [B, T, 3072]                                 │
│              → Linear: [3072 → 3072]                              │
│              (bias=False)                                         │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────┐
│                        SWIGLU MLP DETAIL                                     │
└─────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│                     INPUT: [B, T, 3072]                            │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    FIRST LINEAR LAYER                               │
│                 [3072 → 10752] (3.5× expansion)                   │
│                 Splits into Gate (G1) and Linear (G2)              │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        SPLIT INTO TWO PATHS                         │
│                                                                     │
│    ┌─────────────────┐          ┌─────────────────┐               │
│    │     Gate G1     │          │   Linear G2    │               │
│    │    [B,T,10752]  │          │   [B,T,10752]  │               │
│    └─────────────────┘          └─────────────────┘               │
│            │                            │                         │
│            ▼                            │                         │
│  ┌─────────────────────┐               │                         │
│  │   SiLU Activation   │               │                         │
│  │  (Swish-1)          │               │                         │
│  │  x/(1+exp(-x))      │               │                         │
│  └─────────────────────┘               │                         │
│            │                            │                         │
│            └─────────────┬──────────────┘                         │
│                          │                                      │
│                          ▼                                      │
│            ┌─────────────────────┐                                │
│            │ Element-wise        │                                │
│            │ Multiplication      │                                │
│            │ (Gate × Linear)     │                                │
│            └─────────────────────┘                                │
│                          │                                      │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     SECOND LINEAR LAYER                             │
│                   [10752 → 3072] (compression)                    │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                        DROPOUT (0.1)                               │
│                  During training only                              │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    OUTPUT: [B, T, 3072]                           │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────┐
│                          KEY ARCHITECTURE FEATURES                            │
└─────────────────────────────────────────────────────────────────────────────────┘

SCALING & DIMENSIONS:
┌─────────────────────────────────────────────────────────────────────┐
│ Total Parameters:      ~4,000,000,000                              │
│ Number of Layers:      32                                          │
│ Hidden Dimension:      3072                                        │
│ Attention Heads:       32                                          │
│ Head Dimension:        96 (3072/32)                                │
│ Context Length:        4096 tokens                                 │
│ Vocabulary Size:       ~32,000 (Mistral tokenizer)                │
└─────────────────────────────────────────────────────────────────────┘

NORMALIZATION & ACTIVATION:
┌─────────────────────────────────────────────────────────────────────┐
│ Normalization:         RMSNorm (no centering, learn scale)         │
│ Precision:             bfloat16 training mixed precision            │
│ Dropout:               0.1 throughout                              │
│ Layer Scale:           Optional (init=1e-2) for stability          │
│ MLP Activation:        SwiGLU (SiLU gate)                          │
│ Positional:            Rotary embeddings (RoPE)                    │
│ Attention Bias:        ALiBi (Attention with Linear Biases)        │
└─────────────────────────────────────────────────────────────────────┘

TRAINING INFRASTRUCTURE:
┌─────────────────────────────────────────────────────────────────────┐
│ Distribution:          DDP (torch.distributed)                     │
│ Backend:               NCCL                                         │
│ Compilation:           torch.compile (kernel fusion)               │
│ Scheduler:             Cosine Annealing with Warmup                 │
│ Mixed Precision:       torch.amp (bfloat16)                        │
│ Gradient Clipping:     Enabled for stability                       │
│ Checkpointing:         Regular model checkpoints                     │
└─────────────────────────────────────────────────────────────────────┘

EFFICIENCY FEATURES:
┌─────────────────────────────────────────────────────────────────────┐
│ • 100% PyTorch implementation (no HuggingFace dependencies)        │
│ • Fused QKV projection for memory efficiency                        │
│ • Caching for RoPE position embeddings                             │
│ • Efficient causal mask handling                                   │
│ • Optimized attention kernel (SDPA fallback available)             │
│ • SwiGLU MLP with optimal 3.5× expansion                           │
│ • RMSNorm for faster computation than LayerNorm                    │
│ • Layer scaling for training stability                             │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────┐
│                            DATA FLOW SUMMARY                                │
└─────────────────────────────────────────────────────────────────────────────────┘

Input Tokens → Embedding → Dropout → [32 × (RMSNorm → Attention → LayerScale → 
    Residual → RMSNorm → SwiGLU → LayerScale → Residual)] → Final RMSNorm → 
    Output Head → Logits

Key Design Principles:
✓ Clean, from-scratch PyTorch implementation
✓ Modern transformer optimizations (RoPE, ALiBi, RMSNorm, SwiGLU)
✓ Stable training with layer scaling and proper normalization
✓ Efficient attention with causal masking and distance biases
✓ Scaled for 4B parameters with optimal aspect ratio
✓ Hardware-aware optimizations for AMD GPU training

Performance: 11.0 perplexity on WikiText-2 after 9 days training on 8× AMD MI300X